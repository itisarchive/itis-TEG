{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "429a0a09",
   "metadata": {},
   "source": [
    "# üîç LLMs ‚Äî Response Objects & Unified Interfaces\n",
    "\n",
    "This notebook covers three topics:\n",
    "\n",
    "1. **Azure OpenAI** ‚Äî anatomy of the `ChatCompletion` response object\n",
    "2. **Anthropic Claude** ‚Äî anatomy of the `Message` response object and differences from OpenAI\n",
    "3. **LangChain** ‚Äî unified interface for multiple LLM providers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07eaabb",
   "metadata": {},
   "source": [
    "## 1. Azure OpenAI ‚Äî Response Object Deep Dive\n",
    "\n",
    "Every call to the Chat Completions API returns a rich response object packed with\n",
    "metadata, usage statistics, and ‚Äî of course ‚Äî the generated content itself.\n",
    "\n",
    "**üéØ What You'll Learn:**\n",
    "- The full anatomy of a `ChatCompletion` response object\n",
    "- How to navigate choices, messages, and usage statistics\n",
    "- How to inspect detailed token-usage breakdowns (reasoning, audio, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db433a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai.lib.azure import AzureOpenAI\n",
    "from openai.types.chat import (\n",
    "    ChatCompletionSystemMessageParam,\n",
    "    ChatCompletionUserMessageParam,\n",
    ")\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "azure_openai_client = AzureOpenAI()",
   "id": "1c91f9213500a7a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Response Object Anatomy\n",
    "\n",
    "The Chat Completions API returns a `ChatCompletion` object that contains:\n",
    "* **id** ‚Äî unique identifier of this completion request\n",
    "* **model** ‚Äî the model that actually served the request\n",
    "* **choices[]** ‚Äî list of generated replies (usually one)\n",
    "  * **message** ‚Äî the assistant's message with role and content\n",
    "* **usage** ‚Äî token counts (prompt, completion, total)\n"
   ],
   "id": "25b0220346b8a55c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chat_messages = [\n",
    "    ChatCompletionSystemMessageParam(role=\"system\",\n",
    "                                     content=\"You are a helpful assistant who explains concepts clearly and concisely.\"),\n",
    "    ChatCompletionUserMessageParam(role=\"user\", content=\"Why is the sky blue?\"),\n",
    "]\n",
    "\n",
    "completion_response = azure_openai_client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    messages=chat_messages,\n",
    ")\n",
    "\n",
    "assistant_answer = completion_response.choices[0].message.content.strip()\n",
    "print(assistant_answer)"
   ],
   "id": "d9ebacb26692d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Field-by-Field Exploration",
   "id": "efd6e35c1576a42f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response_object_type = type(completion_response)\n",
    "print(f\"1. Response Object Type:\\n   {response_object_type}\\n\")\n",
    "\n",
    "response_id = completion_response.id\n",
    "print(f\"2. Response ID:\\n   {response_id}\\n\")\n",
    "\n",
    "model_used = completion_response.model\n",
    "print(f\"3. Model Used:\\n   {model_used}\\n\")\n",
    "\n",
    "full_response_repr = completion_response\n",
    "print(f\"4. Full Response Object:\\n   {full_response_repr}\\n\")\n",
    "\n",
    "choices_array = completion_response.choices\n",
    "print(f\"5. Choices Array:\\n   {choices_array}\\n\")\n",
    "\n",
    "first_choice = completion_response.choices[0]\n",
    "print(f\"6. First Choice Object:\\n   {first_choice}\\n\")\n",
    "\n",
    "message_object = first_choice.message\n",
    "print(f\"7. Message Object:\\n   {message_object}\\n\")\n",
    "\n",
    "message_content = message_object.content\n",
    "print(f\"8. Message Content:\\n   {message_content}\\n\")\n",
    "\n",
    "usage_summary = completion_response.usage\n",
    "print(f\"9. Usage Statistics:\\n   {usage_summary}\\n\")"
   ],
   "id": "224b58076d416a00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "completion_response.__dict__",
   "id": "5011c13c22d9e19f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Detailed Usage Statistics\n",
    "\n",
    "Token usage is critical for cost management and performance monitoring.\n",
    "\n",
    "The usage object always includes:\n",
    "* **prompt_tokens** ‚Äî tokens consumed by the input (system + user messages)\n",
    "* **completion_tokens** ‚Äî tokens generated by the model\n",
    "* **total_tokens** ‚Äî sum of the above\n",
    "\n",
    "Some models also expose `completion_tokens_details` with granular breakdowns\n",
    "such as `reasoning_tokens` and `audio_tokens`."
   ],
   "id": "ca5403fb874f7ee8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if completion_response.usage:\n",
    "    prompt_token_count = completion_response.usage.prompt_tokens\n",
    "    completion_token_count = completion_response.usage.completion_tokens\n",
    "    total_token_count = completion_response.usage.total_tokens\n",
    "\n",
    "    print(f\"Prompt tokens:     {prompt_token_count}\")\n",
    "    print(f\"Completion tokens: {completion_token_count}\")\n",
    "    print(f\"Total tokens:      {total_token_count}\")\n",
    "\n",
    "    if hasattr(completion_response.usage, \"completion_tokens_details\"):\n",
    "        token_details = completion_response.usage.completion_tokens_details\n",
    "        if token_details:\n",
    "            print(f\"Reasoning tokens:  {token_details.reasoning_tokens}\")\n",
    "            print(f\"Audio tokens:      {token_details.audio_tokens}\")\n",
    "\n",
    "print(\"\\nüí° The response object contains rich metadata that can be used\")\n",
    "print(\"   for monitoring, logging, and understanding API usage patterns.\")\n"
   ],
   "id": "3ec702773bc52bdd"
  },
  {
   "cell_type": "markdown",
   "id": "7bfee75b",
   "metadata": {},
   "source": [
    "## 2. Anthropic Claude ‚Äî Response Object Deep Dive\n",
    "\n",
    "Every call to the Claude Messages API returns a structured response object with\n",
    "metadata, usage statistics, and the generated content itself.\n",
    "\n",
    "**üéØ What You'll Learn:**\n",
    "- The full anatomy of a Claude `Message` response object\n",
    "- How to navigate content blocks, text, and usage statistics\n",
    "- Key structural differences between the Claude and OpenAI APIs"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import anthropic\n",
    "from anthropic.types import MessageParam, TextBlockParam"
   ],
   "id": "fd2716dd7d078404"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)"
   ],
   "id": "f2558ba4f3ff2393"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Response Object Anatomy\n",
    "\n",
    "The Claude Messages API returns a `Message` object that contains:\n",
    "* **id** ‚Äî unique identifier of this message request\n",
    "* **model** ‚Äî the model that actually served the request\n",
    "* **content[]** ‚Äî list of content blocks (usually one `TextBlock`)\n",
    "  * **.text** ‚Äî the actual generated text\n",
    "* **usage** ‚Äî token counts (`input_tokens`, `output_tokens`)"
   ],
   "id": "3f9925c8468d3a95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "user_message = MessageParam(\n",
    "    role=\"user\",\n",
    "    content=[TextBlockParam(type=\"text\", text=\"Why is the sky blue?\")],\n",
    ")\n",
    "\n",
    "claude_response = anthropic_client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1000,\n",
    "    temperature=1.0,\n",
    "    system=\"You are a friendly assistant answering users' questions. You respond in corporate slang with many anglicisms.\",\n",
    "    messages=[user_message],\n",
    ")\n",
    "\n",
    "assistant_answer_text = claude_response.content[0].text\n",
    "print(textwrap.fill(assistant_answer_text, width=80))"
   ],
   "id": "d22a774529e40115"
  },
  {
   "cell_type": "markdown",
   "id": "bbf2ea91",
   "metadata": {},
   "source": "### Field-by-Field Exploration"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response_object_type = type(claude_response)\n",
    "print(f\"1. Response Object Type:\\n   {response_object_type}\\n\")\n",
    "\n",
    "full_response_repr = claude_response\n",
    "print(f\"2. Full Response Object:\\n   {full_response_repr}\\n\")\n",
    "\n",
    "model_used = claude_response.model\n",
    "print(f\"3. Model Used:\\n   {model_used}\\n\")\n",
    "\n",
    "content_array = claude_response.content\n",
    "print(f\"4. Content Array:\\n   {content_array}\\n\")\n",
    "\n",
    "first_content_block = claude_response.content[0]\n",
    "print(f\"5. First Content Block:\\n   {first_content_block}\\n\")\n",
    "\n",
    "first_content_block_text = claude_response.content[0].text\n",
    "print(f\"6. First Content Block Text:\\n   {first_content_block_text}\\n\")\n",
    "\n",
    "usage_summary = claude_response.usage\n",
    "print(f\"7. Usage Statistics:\\n   {usage_summary}\\n\")"
   ],
   "id": "1248981649998817"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Detailed Usage Statistics\n",
    "\n",
    "Token usage is critical for cost management and performance monitoring.\n",
    "\n",
    "The Claude usage object includes:\n",
    "* **input_tokens** ‚Äî tokens consumed by the input (system + user messages)\n",
    "* **output_tokens** ‚Äî tokens generated by the model\n",
    "\n",
    "Note: Unlike OpenAI, Claude does not provide a `total_tokens` field,\n",
    "so we compute it ourselves."
   ],
   "id": "372b71ef7316cd40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if claude_response.usage:\n",
    "    input_token_count = claude_response.usage.input_tokens\n",
    "    output_token_count = claude_response.usage.output_tokens\n",
    "    total_token_count = input_token_count + output_token_count\n",
    "\n",
    "    print(f\"Input tokens:  {input_token_count}\")\n",
    "    print(f\"Output tokens: {output_token_count}\")\n",
    "    print(f\"Total tokens:  {total_token_count}\")"
   ],
   "id": "88711403939ffd7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Claude vs OpenAI ‚Äî Structural Differences\n",
    "\n",
    "Claude and OpenAI APIs share similar concepts but differ in structure.\n",
    "Understanding these differences is key when switching between providers.\n",
    "\n",
    "| Aspect         | Claude                           | OpenAI                                |\n",
    "|----------------|----------------------------------|---------------------------------------|\n",
    "| Method         | `messages.create()`              | `chat.completions.create()`           |\n",
    "| Content path   | `content[0].text`                | `choices[0].message.content`          |\n",
    "| System message | separate `system` parameter      | part of `messages` array              |\n",
    "| Usage fields   | `input_tokens` / `output_tokens` | `prompt_tokens` / `completion_tokens` |\n"
   ],
   "id": "ac041732d763d396"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. LangChain ‚Äî Unified Interface for Multiple Providers\n",
    "\n",
    "LangChain provides a single abstraction layer so you can swap LLM providers\n",
    "without rewriting application logic.\n",
    "\n",
    "**üéØ What You'll Learn:**\n",
    "- How LangChain wraps Azure OpenAI and Claude behind a common interface\n",
    "- How to use Ollama for local model inference via LangChain\n",
    "- Why a unified abstraction matters for swapping models painlessly"
   ],
   "id": "8c66c609eb8e5961"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from pydantic import SecretStr"
   ],
   "id": "bb1f9db3a9cec169"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Azure OpenAI ‚Äî via LangChain\n",
    "\n",
    "LangChain wraps Azure OpenAI behind a unified `ChatModel` interface.\n",
    "Calling `.invoke(question)` returns an `AIMessage` ‚Äî same shape regardless of provider."
   ],
   "id": "cf2c612c335f44c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "azure_langchain_llm = AzureChatOpenAI(model=\"gpt-4o-mini\")\n",
    "sky_question = \"Why is the sky blue?\"\n",
    "azure_langchain_response = azure_langchain_llm.invoke(sky_question)\n",
    "print(azure_langchain_response.content)"
   ],
   "id": "57ce5e25ad551185"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude ‚Äî via LangChain\n",
    "\n",
    "The same `.invoke()` call works identically for Claude, making it trivial\n",
    "to swap providers without touching application logic."
   ],
   "id": "dc4a3c9c82f38dc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "anthropic_api_key_secret = SecretStr(os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "claude_langchain_llm = ChatAnthropic(\n",
    "    model_name=\"claude-3-5-sonnet-20241022\",\n",
    "    api_key=anthropic_api_key_secret,\n",
    "    timeout=30,\n",
    "    stop=[\"\\n\\nHuman:\", \"\\n\\nAssistant:\"],\n",
    ")\n",
    "sky_question = \"Why is the sky blue?\"\n",
    "claude_langchain_response = claude_langchain_llm.invoke(sky_question)\n",
    "print(claude_langchain_response.content)"
   ],
   "id": "3138a4b4830de53a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zadanie\n",
    "\n",
    "Przetestowaƒá po≈ÇƒÖczenie z lokalnymi modelami LLM poprzez Ollama przy u≈ºyciu biblioteki LangChain.\n",
    "\n",
    "- üì• Instalacja Ollama: https://ollama.com/download/linux\n",
    "- üè∑Ô∏è Przyk≈Çadowy model: https://ollama.com/library/gemma3:1b\n",
    "\n",
    "üìö Dokumentacja LangChain: https://docs.langchain.com/oss/python/langchain/overview\n"
   ],
   "id": "4d0522eb9ca0ac76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
