#!/usr/bin/env python3
"""
ðŸ” Anthropic Claude Response Object â€” Deep Dive
=================================================

Every call to the Claude Messages API returns a structured response object with
metadata, usage statistics, and the generated content itself.

This script walks you through each layer of that object so you can confidently
extract exactly the data you need â€” and understand how it differs from OpenAI.

ðŸŽ¯ What You'll Learn:
- The full anatomy of a Claude Message response object
- How to navigate content blocks, text, and usage statistics
- Key structural differences between the Claude and OpenAI APIs

ðŸ”§ Prerequisites:
- ANTHROPIC_API_KEY in .env file
- Python 3.13+ with anthropic and python-dotenv packages
"""

import os
import textwrap

import anthropic
from anthropic.types import Message, MessageParam, TextBlockParam
from dotenv import load_dotenv


def print_section_header(title: str) -> None:
    separator = "=" * 60
    print(f"\n{separator}\n{title}\n{separator}")


def send_claude_message(
        claude_client: anthropic.Anthropic,
        *,
        system_prompt: str,
        user_prompt: str,
        model_name: str = "claude-sonnet-4-20250514",
        max_tokens: int = 1000,
        temperature: float = 1.0,
) -> Message:
    """Sends a system+user message pair to Claude and returns the raw Message object."""
    user_message = MessageParam(
        role="user",
        content=[TextBlockParam(type="text", text=user_prompt)],
    )
    return claude_client.messages.create(
        model=model_name,
        max_tokens=max_tokens,
        temperature=temperature,
        system=system_prompt,
        messages=[user_message],
    )


def demonstrate_response_object_anatomy(claude_client: anthropic.Anthropic) -> None:
    """
    The Claude Messages API returns a Message object that contains:
    â€¢ id            â€” unique identifier of this message request
    â€¢ model         â€” the model that actually served the request
    â€¢ content[]     â€” list of content blocks (usually one TextBlock)
      â”” .text       â€” the actual generated text
    â€¢ usage         â€” token counts (input_tokens, output_tokens)

    Below we make a single request and then unpack every layer of the response
    so you can see what lives inside.
    """
    print_section_header("RESPONSE OBJECT ANATOMY")
    print(textwrap.dedent(demonstrate_response_object_anatomy.__doc__))

    claude_response = send_claude_message(
        claude_client,
        system_prompt=(
            "You are a friendly assistant answering users' questions. "
            "You respond in corporate slang with many anglicisms."
        ),
        user_prompt="Why is the sky blue?",
    )

    assistant_answer_text = claude_response.content[0].text
    print(textwrap.fill(assistant_answer_text, width=80))

    print_section_header("FIELD-BY-FIELD EXPLORATION")

    response_object_type = type(claude_response)
    print(f"1. Response Object Type:\n   {response_object_type}\n")

    full_response_repr = claude_response
    print(f"2. Full Response Object:\n   {full_response_repr}\n")

    model_used = claude_response.model
    print(f"3. Model Used:\n   {model_used}\n")

    content_array = claude_response.content
    print(f"4. Content Array:\n   {content_array}\n")

    first_content_block = claude_response.content[0]
    print(f"5. First Content Block:\n   {first_content_block}\n")

    first_content_block_text = claude_response.content[0].text
    print(f"6. First Content Block Text:\n   {first_content_block_text}\n")

    usage_summary = claude_response.usage
    print(f"7. Usage Statistics:\n   {usage_summary}\n")

    display_detailed_usage_statistics(claude_response)
    display_api_comparison_with_openai()


def display_detailed_usage_statistics(claude_response: Message) -> None:
    """
    Token usage is critical for cost management and performance monitoring.

    The Claude usage object includes:
    â€¢ input_tokens   â€” tokens consumed by the input (system + user messages)
    â€¢ output_tokens  â€” tokens generated by the model

    Note: Unlike OpenAI, Claude does not provide a 'total_tokens' field,
    so we compute it ourselves.
    """
    if not claude_response.usage:
        return

    print_section_header("DETAILED USAGE STATISTICS")
    print(textwrap.dedent(display_detailed_usage_statistics.__doc__))

    input_token_count = claude_response.usage.input_tokens
    output_token_count = claude_response.usage.output_tokens
    total_token_count = input_token_count + output_token_count

    print(textwrap.dedent(f"""\
        Input tokens:  {input_token_count}
        Output tokens: {output_token_count}
        Total tokens:  {total_token_count}"""))


def display_api_comparison_with_openai() -> None:
    """
    Claude and OpenAI APIs share similar concepts but differ in structure.
    Understanding these differences is key when switching between providers.

    Key differences:
    â€¢ Method:       messages.create()  vs  chat.completions.create()
    â€¢ Content path: content[0].text    vs  choices[0].message.content
    â€¢ System msg:   separate parameter vs  part of messages array
    â€¢ Usage fields: input_tokens / output_tokens  vs  prompt_tokens / completion_tokens
    """
    print_section_header("CLAUDE vs OPENAI â€” STRUCTURAL DIFFERENCES")
    print(textwrap.dedent(display_api_comparison_with_openai.__doc__))


if __name__ == "__main__":
    load_dotenv(override=True)
    anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
    anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)

    demonstrate_response_object_anatomy(anthropic_client)
