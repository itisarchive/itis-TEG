#!/usr/bin/env python3
"""
ðŸ” Azure OpenAI Response Object â€” Deep Dive
============================================

Every call to the Chat Completions API returns a rich response object packed with
metadata, usage statistics, and â€” of course â€” the generated content itself.

This script walks you through each layer of that object so you can confidently
extract exactly the data you need for logging, monitoring, and cost tracking.

ðŸŽ¯ What You'll Learn:
- The full anatomy of a ChatCompletion response object
- How to navigate choices, messages, and usage statistics
- How to inspect detailed token-usage breakdowns (reasoning, audio, etc.)

ðŸ”§ Prerequisites:
- Azure OpenAI credentials in .env file
- Python 3.13+ with openai and python-dotenv packages
"""

import textwrap

from dotenv import load_dotenv
from openai.lib.azure import AzureOpenAI
from openai.types.chat import ChatCompletion
from openai.types.chat import (
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
)


def print_section_header(title: str) -> None:
    separator = "=" * 60
    print(f"\n{separator}\n{title}\n{separator}")


def send_chat_completion(
        client: AzureOpenAI,
        *,
        system_prompt: str,
        user_prompt: str,
        model_name: str = "gpt-5-nano",
) -> ChatCompletion:
    """Sends a system+user message pair to Azure OpenAI and returns the raw ChatCompletion object."""
    chat_messages = [
        ChatCompletionSystemMessageParam(role="system", content=system_prompt),
        ChatCompletionUserMessageParam(role="user", content=user_prompt),
    ]
    return client.chat.completions.create(
        model=model_name,
        messages=chat_messages,
    )


def demonstrate_response_object_anatomy(client: AzureOpenAI) -> None:
    """
    The Chat Completions API returns a ChatCompletion object that contains:
    â€¢ id            â€” unique identifier of this completion request
    â€¢ model         â€” the model that actually served the request
    â€¢ choices[]     â€” list of generated replies (usually one)
      â”” message     â€” the assistant's message with role and content
    â€¢ usage         â€” token counts (prompt, completion, total)

    Below we make a single request and then unpack every layer of the response
    so you can see what lives inside.
    """
    print_section_header("RESPONSE OBJECT ANATOMY")
    print(textwrap.dedent(demonstrate_response_object_anatomy.__doc__))

    completion_response = send_chat_completion(
        client,
        system_prompt="You are a helpful assistant who explains concepts clearly and concisely.",
        user_prompt="Why is the sky blue?",
    )

    assistant_answer = completion_response.choices[0].message.content.strip()
    print(assistant_answer)

    print_section_header("FIELD-BY-FIELD EXPLORATION")

    response_object_type = type(completion_response)
    print(f"1. Response Object Type:\n   {response_object_type}\n")

    response_id = completion_response.id
    print(f"2. Response ID:\n   {response_id}\n")

    model_used = completion_response.model
    print(f"3. Model Used:\n   {model_used}\n")

    full_response_repr = completion_response
    print(f"4. Full Response Object:\n   {full_response_repr}\n")

    choices_array = completion_response.choices
    print(f"5. Choices Array:\n   {choices_array}\n")

    first_choice = completion_response.choices[0]
    print(f"6. First Choice Object:\n   {first_choice}\n")

    message_object = first_choice.message
    print(f"7. Message Object:\n   {message_object}\n")

    message_content = message_object.content
    print(f"8. Message Content:\n   {message_content}\n")

    usage_summary = completion_response.usage
    print(f"9. Usage Statistics:\n   {usage_summary}\n")

    display_detailed_usage_statistics(completion_response)

    print(textwrap.dedent("""\

        ðŸ’¡ The response object contains rich metadata that can be used
           for monitoring, logging, and understanding API usage patterns."""))


def display_detailed_usage_statistics(completion_response: ChatCompletion) -> None:
    """
    Token usage is critical for cost management and performance monitoring.

    The usage object always includes:
    â€¢ prompt_tokens      â€” tokens consumed by the input (system + user messages)
    â€¢ completion_tokens  â€” tokens generated by the model
    â€¢ total_tokens       â€” sum of the above

    Some models also expose completion_tokens_details with granular breakdowns
    such as reasoning_tokens and audio_tokens.
    """
    if not completion_response.usage:
        return

    print_section_header("DETAILED USAGE STATISTICS")
    print(textwrap.dedent(display_detailed_usage_statistics.__doc__))

    prompt_token_count = completion_response.usage.prompt_tokens
    completion_token_count = completion_response.usage.completion_tokens
    total_token_count = completion_response.usage.total_tokens

    print(textwrap.dedent(f"""\
        Prompt tokens:     {prompt_token_count}
        Completion tokens: {completion_token_count}
        Total tokens:      {total_token_count}"""))

    if hasattr(completion_response.usage, "completion_tokens_details"):
        token_details = completion_response.usage.completion_tokens_details
        if token_details:
            print(f"Reasoning tokens:  {token_details.reasoning_tokens}")
            print(f"Audio tokens:      {token_details.audio_tokens}")


if __name__ == "__main__":
    load_dotenv(override=True)
    azure_openai_client = AzureOpenAI()

    demonstrate_response_object_anatomy(azure_openai_client)
